{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-03T03:16:02.940083500Z",
     "start_time": "2023-10-03T03:16:01.222204Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import save_model, load_model\n",
    "from transformers.models.bert import TFBertModel, BertTokenizer\n",
    "from transformers.models.gpt2 import TFGPT2Model, GPT2Tokenizer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from rouge import Rouge\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HybridModel(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 bert_model_name='bert-base-uncased', \n",
    "                 gpt2_model_name='gpt2', \n",
    "                 learning_rate=1e-3,\n",
    "                 dropout_rate=0.1, \n",
    "                 regularization_factor=0.01,\n",
    "                 loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        # BERT Encoder\n",
    "        self.bert_encoder = TFBertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Extract hidden dimensions from BERT and GPT-2 models\n",
    "        bert_hidden_dim = self.bert_encoder.config.hidden_size\n",
    "        \n",
    "        # GPT-2 Decoder\n",
    "        self.gpt2_decoder = TFGPT2Model.from_pretrained(gpt2_model_name)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "        \n",
    "        gpt2_hidden_dim = self.gpt2_decoder.config.n_embd\n",
    "        \n",
    "        # Intermediate Layer: Map BERT's hidden dimensions to GPT-2's hidden dimensions\n",
    "        self.intermediate = tf.keras.layers.Dense(gpt2_hidden_dim)\n",
    "        \n",
    "        self.loss_fn = loss_function\n",
    "        self.train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
    "        self.eval_loss_metric = tf.keras.metrics.Mean(name='eval_loss')\n",
    "        \n",
    "        # Initialize TensorBoard writer\n",
    "        self.writer = tf.summary.create_file_writer('logs/')\n",
    "\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None):\n",
    "        # Forward pass through BERT encoder\n",
    "        bert_outputs = self.bert_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        # Extract the [CLS] embeddings or average all embeddings\n",
    "        bert_cls_output = bert_outputs[:, 0, :]\n",
    "        \n",
    "        # Intermediate layer\n",
    "        intermediate_output = self.intermediate(bert_cls_output)\n",
    "        intermediate_output = self.batch_norm(intermediate_output) \n",
    "        \n",
    "        # Forward pass through GPT-2 decoder\n",
    "        gpt2_outputs = self.gpt2_decoder(inputs_embeds=intermediate_output)[0]\n",
    "        \n",
    "        return gpt2_outputs\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        # Unpack the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self(input_ids, attention_mask)\n",
    "        \n",
    "        # Reshape target_ids and output for loss computation\n",
    "        target_ids = tf.reshape(target_ids, [-1])\n",
    "        output = tf.reshape(output, [-1, output.shape[-1]])\n",
    "        \n",
    "        # Compute loss using Cross-Entropy\n",
    "        loss = self.loss_fn(target_ids, output)\n",
    "        \n",
    "        # Update metric and log\n",
    "        self.train_loss_metric.update_state(loss)\n",
    "        \n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('train_loss', self.train_loss_metric.result(), step=batch['step'])\n",
    "        \n",
    "        return {'loss': loss}  # Additional info can be added to the dictionary if needed\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        # Unpack the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self(input_ids, attention_mask)\n",
    "        \n",
    "        # Reshape target_ids and output for loss computation\n",
    "        target_ids = tf.reshape(target_ids, [-1])\n",
    "        output = tf.reshape(output, [-1, output.shape[-1]])\n",
    "        \n",
    "        # Compute loss using Cross-Entropy\n",
    "        val_loss = self.loss_fn(target_ids, output)\n",
    "        \n",
    "        # Update metric and log\n",
    "        self.val_loss_metric.update_state(val_loss)\n",
    "        \n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('val_loss', self.val_loss_metric.result(), step=batch['step'])\n",
    "        \n",
    "        # Return the validation loss and additional information as a dictionary (optional)\n",
    "        return {'val_loss': val_loss}  # Additional info can be added to the dictionary if needed\n",
    "\n",
    "    \n",
    "    def evaluation_step(self, batch):\n",
    "        # Unpack the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "    \n",
    "        # Forward pass\n",
    "        output = self(input_ids, attention_mask)\n",
    "        \n",
    "        # Reshape target_ids and output for loss computation\n",
    "        target_ids = tf.reshape(target_ids, [-1])\n",
    "        output = tf.reshape(output, [-1, output.shape[-1]])\n",
    "    \n",
    "        # Compute loss using Cross-Entropy\n",
    "        eval_loss = self.loss_fn(target_ids, output)\n",
    "    \n",
    "        # Decode the model's predictions to text\n",
    "        predicted_summary = self.gpt2_tokenizer.decode(tf.argmax(output, axis=-1), skip_special_tokens=True)\n",
    "        \n",
    "        # Decode the target summary to text\n",
    "        target_summary = self.gpt2_tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predicted_summary, target_summary, avg=True)\n",
    "        \n",
    "        # Log validation loss and ROUGE score\n",
    "        with tf.summary.create_file_writer('logs').as_default():\n",
    "            tf.summary.scalar('val_loss', self.val_loss_metric.result(), step=batch['step'])\n",
    "            tf.summary.scalar('ROUGE-1-score', scores['rouge-1']['f'], step=batch['step'])\n",
    "            tf.summary.scalar('ROUGE-2-score', scores['rouge-2']['f'], step=batch['step'])\n",
    "            tf.summary.scalar('ROUGE-L-score', scores['rouge-l']['f'], step=batch['step'])\n",
    "    \n",
    "        # Return the evaluation loss and ROUGE scores as a dictionary (optional)\n",
    "        return {'eval_loss': eval_loss, 'rouge_scores': scores}\n",
    "        \n",
    "    \n",
    "    def save_checkpoint(self, filepath):\n",
    "        self.save_weights(filepath)\n",
    "        # TODO: include other relevant information, if needed\n",
    "\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, filepath, optimizer=None):\n",
    "        model = cls()  # Initialize a model instance of the class\n",
    "        model.load_weights(filepath)\n",
    "\n",
    "        # For optimizer, explicit handling may be needed if optimizer state is important\n",
    "        # In general, TensorFlow optimizers are often re-initialized during training\n",
    "\n",
    "        # Retrieve other saved information (if saved separately)\n",
    "        # other_info = ...\n",
    "\n",
    "        return model, optimizer  # Additional info can be added if needed\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86266a6326908af1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class HybridModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 bert_model_name='bert-base-uncased',\n",
    "                 gpt2_model_name='gpt2',\n",
    "                 learning_rate=1e-3,\n",
    "                 dropout_rate=0.1,\n",
    "                 regularization_factor=0.01,\n",
    "                 loss_function=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.bert_encoder = TFBertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        bert_hidden_dim = self.bert_encoder.config.hidden_size\n",
    "        self.gpt2_decoder = TFGPT2Model.from_pretrained(gpt2_model_name)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "        gpt2_hidden_dim = self.gpt2_decoder.config.n_embd\n",
    "        # Initialize intermediate dense layer with L2 regularization\n",
    "        self.intermediate = tf.keras.layers.Dense(gpt2_hidden_dim,\n",
    "                                                 kernel_regularizer=l2(regularization_factor))\n",
    "        self.loss_fn = loss_function\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
    "        self.eval_loss_metric = tf.keras.metrics.Mean(name='eval_loss')\n",
    "        self.writer = tf.summary.create_file_writer('logs/')\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def call(self, input_ids, attention_mask=None, training=False):\n",
    "        bert_outputs = self.bert_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "        bert_cls_output = bert_outputs[:, 0, :]\n",
    "        intermediate_output = self.intermediate(bert_cls_output)\n",
    "        intermediate_output = self.batch_norm(intermediate_output, training)\n",
    "        gpt2_outputs = self.gpt2_decoder(inputs_embeds=intermediate_output)[0]\n",
    "        return gpt2_outputs\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        output = self(input_ids, attention_mask)\n",
    "        target_ids = tf.reshape(target_ids, [-1])\n",
    "        output = tf.reshape(output, [-1, output.shape[-1]])\n",
    "        loss = self.loss_fn(target_ids, output)\n",
    "        self.train_loss_metric.update_state(loss)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('train_loss', self.train_loss_metric.result(), step=batch['step'])\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        output = self(input_ids, attention_mask)\n",
    "        target_ids = tf.reshape(target_ids, [-1])\n",
    "        output = tf.reshape(output, [-1, output.shape[-1]])\n",
    "        val_loss = self.loss_fn(target_ids, output)\n",
    "        self.val_loss_metric.update_state(val_loss)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('val_loss', self.val_loss_metric.result(), step=batch['step'])\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def evaluation_step(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        output = self(input_ids, attention_mask)\n",
    "        target_ids = tf.reshape(target_ids, [-1])\n",
    "        output = tf.reshape(output, [-1, output.shape[-1]])\n",
    "        eval_loss = self.loss_fn(target_ids, output)\n",
    "        predicted_summary = self.gpt2_tokenizer.decode(tf.argmax(output, axis=-1), skip_special_tokens=True)\n",
    "        target_summary = self.gpt2_tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predicted_summary, target_summary, avg=True)\n",
    "        with tf.summary.create_file_writer('logs').as_default():\n",
    "            tf.summary.scalar('val_loss', self.val_loss_metric.result(), step=batch['step'])\n",
    "            tf.summary.scalar('ROUGE-1-score', scores['rouge-1']['f'], step=batch['step'])\n",
    "            tf.summary.scalar('ROUGE-2-score', scores['rouge-2']['f'], step=batch['step'])\n",
    "            tf.summary.scalar('ROUGE-L-score', scores['rouge-l']['f'], step=batch['step'])\n",
    "        return {'eval_loss': eval_loss, 'rouge_scores': scores}\n",
    "\n",
    "    def save_checkpoint(self, filepath, optimizer):\n",
    "        self.save_weights(filepath)\n",
    "        with open(filepath + '_optimizer.pkl', 'wb') as f:\n",
    "            pickle.dump(optimizer.get_weights(), f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, filepath, optimizer):\n",
    "        model = cls() \n",
    "        model.load_weights(filepath)\n",
    "        with open(filepath + '_optimizer.pkl', 'rb') as f:\n",
    "            optimizer_weights = pickle.load(f)\n",
    "        optimizer.set_weights(optimizer_weights)\n",
    "        return model, optimizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T03:15:13.262846100Z",
     "start_time": "2023-10-03T03:15:13.258845800Z"
    }
   },
   "id": "db4c49a8c2f3cc9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-gpu",
   "language": "python",
   "display_name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
