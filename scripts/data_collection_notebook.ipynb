{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-18T00:07:18.369188200Z",
     "start_time": "2023-09-18T00:07:18.367187900Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, DatasetSearchArguments, DatasetFilter\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T03:00:12.805330700Z",
     "start_time": "2023-09-17T03:00:12.799329600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "# cnn_dataset.save_to_disk(\"datasets/cnn_dailymail\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T03:00:12.810329800Z",
     "start_time": "2023-09-17T03:00:12.805330700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object HfApi.list_datasets at 0x000002A96C114C80>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# List all datasets\n",
    "api.list_datasets()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T03:00:12.811329600Z",
     "start_time": "2023-09-17T03:00:12.805330700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Get all valid search arguments\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mDatasetSearchArguments\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\huggingface_hub\\hf_api.py:665\u001B[0m, in \u001B[0;36mDatasetSearchArguments.__init__\u001B[1;34m(self, api)\u001B[0m\n\u001B[0;32m    663\u001B[0m tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mget_dataset_tags()\n\u001B[0;32m    664\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(tags)\n\u001B[1;32m--> 665\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\huggingface_hub\\hf_api.py:673\u001B[0m, in \u001B[0;36mDatasetSearchArguments._process_models\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m datasets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mlist_datasets()\n\u001B[0;32m    672\u001B[0m author_dict, dataset_name_dict \u001B[38;5;241m=\u001B[39m AttributeDictionary(), AttributeDictionary()\n\u001B[1;32m--> 673\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mid:\n\u001B[0;32m    675\u001B[0m         author, name \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mid\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\huggingface_hub\\hf_api.py:1302\u001B[0m, in \u001B[0;36mHfApi.list_datasets\u001B[1;34m(self, filter, author, search, sort, direction, limit, full, token)\u001B[0m\n\u001B[0;32m   1300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1301\u001B[0m     items \u001B[38;5;241m=\u001B[39m islice(items, limit)  \u001B[38;5;66;03m# Do not iterate over all pages\u001B[39;00m\n\u001B[1;32m-> 1302\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m items:\n\u001B[0;32m   1303\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m DatasetInfo(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mitem)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:44\u001B[0m, in \u001B[0;36mpaginate\u001B[1;34m(path, params, headers)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m next_page \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     43\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPagination detected. Requesting next page: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnext_page\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 44\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_page\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m     hf_raise_for_status(r)\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m r\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\requests\\sessions.py:600\u001B[0m, in \u001B[0;36mSession.get\u001B[1;34m(self, url, **kwargs)\u001B[0m\n\u001B[0;32m    592\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[0;32m    593\u001B[0m \n\u001B[0;32m    594\u001B[0m \u001B[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001B[39;00m\n\u001B[0;32m    596\u001B[0m \u001B[38;5;124;03m:rtype: requests.Response\u001B[39;00m\n\u001B[0;32m    597\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    599\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 600\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\requests\\sessions.py:587\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    582\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    584\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    585\u001B[0m }\n\u001B[0;32m    586\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 587\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\requests\\sessions.py:701\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    698\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    700\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    704\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:63\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[1;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     65\u001B[0m     request_id \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\requests\\adapters.py:487\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    484\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 487\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_proxy(conn)\n\u001B[0;32m    713\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[1;32m--> 714\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[0;32m    726\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n\u001B[0;32m    728\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\urllib3\\connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    461\u001B[0m             httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m--> 466\u001B[0m             \u001B[43msix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\urllib3\\connectionpool.py:461\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;66;03m# Python 3\u001B[39;00m\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 461\u001B[0m         httplib_response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    462\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m         \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m         \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m         \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m    466\u001B[0m         six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\http\\client.py:1348\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1347\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1348\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1349\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1350\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\http\\client.py:316\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 316\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    318\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\http\\client.py:277\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 277\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    279\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1238\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1239\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1240\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TensorFlow-GPU\\lib\\ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all valid search arguments\n",
    "args = DatasetSearchArguments()\n",
    "\n",
    "\n",
    "def get_tag_value(tag_name, dataset):\n",
    "    # Initialize an empty list to store the tag values\n",
    "    tag_values = []\n",
    "\n",
    "    # Loop through the tags of each item\n",
    "    for tag in dataset.tags:\n",
    "        # Split the tag into its name and value\n",
    "        tag_parts = tag.split(\":\")\n",
    "\n",
    "        # Check if the tag name matches the one you are interested in\n",
    "        if tag_parts[0] == tag_name:\n",
    "            # Append the value part of the tag to the list\n",
    "            tag_values.append(tag_parts[1])\n",
    "\n",
    "    return tag_values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T03:00:20.495708500Z",
     "start_time": "2023-09-17T03:00:12.813330Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create the filters with for the database request\n",
    "filt = DatasetFilter(task_categories=args.task_categories.summarization, language='en', multilinguality=\"monolingual\")\n",
    "my_list = api.list_datasets(filter=filt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize sets for tags and classifications\n",
    "my_tags = set()\n",
    "my_classifications = set()\n",
    "\n",
    "# Loop through your list\n",
    "for item in my_list:\n",
    "    print(item)\n",
    "    for tag in item.tags:\n",
    "        # If the tag contains a colon, it has sub-information\n",
    "        if \":\" in tag:\n",
    "            my_tags.add(tag.split(\":\")[0])\n",
    "        # Otherwise, it is a classification\n",
    "        else:\n",
    "            my_classifications.add(tag)\n",
    "\n",
    "print(\"Tags:\", my_tags)\n",
    "print(\"Classifications:\", my_classifications)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_items = []\n",
    "for item in my_list:\n",
    "    for tag in item.tags:\n",
    "        tagItem = tag.split(\":\")\n",
    "        if tagItem[0].lower() == \"license\":\n",
    "            print(f\"{item.id} license: {tagItem[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(datasets.get_dataset_config_info(\"cnn_dailymail\", \"3.0.0\", download_mode='reuse_cache_if_exists'))\n",
    "print(datasets.get_dataset_config_info(\"cnn_dailymail\", \"2.0.0\", download_mode='reuse_cache_if_exists'))\n",
    "print(datasets.get_dataset_config_info(\"cnn_dailymail\", \"1.0.0\", download_mode='reuse_cache_if_exists'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "filt = DatasetFilter(task_categories=args.task_categories.summarization, language='en', multilinguality=\"monolingual\")\n",
    "my_list = api.list_datasets(filter=filt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T04:10:50.237037200Z",
     "start_time": "2023-09-17T04:10:50.235037200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Look out features on different datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroNLP/ik-nlp-22_slp skipped unknown reason: Python int too large to convert to C long\n",
      "Hellisotherpeople/DebateSum skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "ccdv/cnn_dailymail skipped unknown reason: The split names could not be parsed from the dataset config.\n",
      "DebateLabKIT/aaac skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "lukesjordan/worldbank-project-documents skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "stevhliu/demo skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "shanya/crd3 skipped unknown reason: The split names could not be parsed from the dataset config.\n",
      "alexfabbri/answersumm skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "knkarthick/dialogsum skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "knkarthick/AMI skipped unknown reason: Couldn't find a dataset script at C:\\Users\\PC\\PycharmProjects\\TLDR\\scripts\\knkarthick\\AMI\\AMI.py or any data file in the same directory. Couldn't find 'knkarthick/AMI' on the Hugging Face Hub either: FileNotFoundError: Dataset 'knkarthick/AMI' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.\n",
      "knkarthick/samsum skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "knkarthick/xsum skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "knkarthick/highlightsum skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "knkarthick/topicsum skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "elihoole/asrs-aviation-reports skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "RUCAIBox/Summarization skipped unknown reason: Couldn't find a dataset script at C:\\Users\\PC\\PycharmProjects\\TLDR\\scripts\\RUCAIBox\\Summarization\\Summarization.py or any data file in the same directory. Couldn't find 'RUCAIBox/Summarization' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in RUCAIBox/Summarization. \n",
      "gaurikapse/civis-consultation-summaries skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "RamAnanth1/lex-fridman-podcasts skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "dreamproit/bill_summary_us skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "Axel578/mydt skipped unknown reason: 'NoneType' object has no attribute 'keys'\n",
      "NavidVafaei/rottentomato01 skipped unknown reason: array() takes at least 1 positional argument (0 given)\n",
      "scillm/scientific_papers skipped unknown reason: 'NoneType' object has no attribute 'keys'\n"
     ]
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43dc35ec152e46f1b4a159f3ed96d785"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66e57e9224b4461095577b70fa9a169b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neil-code/dialogsum-test skipped unknown reason: 'NoneType' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": [
    "features_configurations = set()\n",
    "for item in my_list:\n",
    "    try:\n",
    "        config_name = datasets.get_dataset_config_names(item.id, download_mode='reuse_dataset_if_exists')\n",
    "        if config_name:\n",
    "            for config in config_name:\n",
    "                configs = datasets.get_dataset_config_info(item.id, config, download_mode='reuse_dataset_if_exists')\n",
    "                if configs.features is not None:\n",
    "                    features_configurations.add(frozenset(configs.features.keys()))\n",
    "        else:\n",
    "            configs = datasets.get_dataset_config_info(item.id, download_mode='reuse_dataset_if_exists')\n",
    "            if configs.features is not None:\n",
    "                features_configurations.add(frozenset(configs.features.keys()))\n",
    "    except Exception as e:\n",
    "        print(f\"{item.id} skipped unknown reason: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T03:31:49.314188100Z",
     "start_time": "2023-09-17T03:26:03.147782900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{frozenset({'abstract', 'description'}),\n frozenset({'compression',\n            'compression_bin',\n            'coverage',\n            'coverage_bin',\n            'date',\n            'density',\n            'density_bin',\n            'summary',\n            'text',\n            'title',\n            'url'}),\n frozenset({'doc_id', 'document_sections', 'question_summary_pairs'}),\n frozenset({'abstract', 'article', 'section_names'}),\n frozenset({'directions',\n            'id',\n            'ingredients',\n            'link',\n            'ner',\n            'source',\n            'title'}),\n frozenset({'category', 'content', 'headline'}),\n frozenset({'CHQ', 'File', 'Summary'}),\n frozenset({'doc_id',\n            'document_sections',\n            'question_summary_pairs',\n            'summary_paragraph_index'}),\n frozenset({'id', 'sources', 'summary/long', 'summary/short', 'summary/tiny'}),\n frozenset({'gem_id',\n            'gem_parent_id',\n            'meaning_representation',\n            'references',\n            'target'}),\n frozenset({'language',\n            'product_category',\n            'product_id',\n            'review_body',\n            'review_id',\n            'review_title',\n            'reviewer_id',\n            'stars'}),\n frozenset({'comparison_preference',\n            'feedback',\n            'feedback_class',\n            'generated_summary_for_comparison_A',\n            'generated_summary_for_comparison_B',\n            'generated_summary_for_feedback',\n            'has_additional_feedback',\n            'id',\n            'ideal_human_summary',\n            'post',\n            'subreddit',\n            'summary_prompt',\n            'time_spent_in_seconds_comparison',\n            'time_spent_in_seconds_feedback',\n            'time_spent_in_seconds_ideal_human_summary',\n            'title',\n            'tldr_human_reference_summary'}),\n frozenset({'exid', 'inputs', 'targets'}),\n frozenset({'article', 'step_headers', 'summary', 'title', 'url'}),\n frozenset({'document', 'id', 'summary'}),\n frozenset({'abstract',\n            'authors',\n            'categories',\n            'comments',\n            'doi',\n            'id',\n            'journal-ref',\n            'license',\n            'report-no',\n            'submitter',\n            'title',\n            'update_date'}),\n frozenset({'id', 'summary', 'transcript'}),\n frozenset({'abstract', 'pmid', 'review_id', 'target', 'title'}),\n frozenset({'directions', 'ingredients', 'misc', 'ner', 'source', 'title'}),\n frozenset({'abstract', 'aid', 'mid', 'ref_abstract', 'related_work'}),\n frozenset({'document_sections', 'id', 'summary_sections'}),\n frozenset({'dialog_act',\n            'dialog_act_delexicalized',\n            'gem_id',\n            'gem_parent_id',\n            'references',\n            'target',\n            'target_delexicalized'}),\n frozenset({'article',\n            'keywords',\n            'section_headings',\n            'summary',\n            'title',\n            'year'}),\n frozenset({'case_metadata',\n            'id',\n            'sources',\n            'sources_metadata',\n            'summary/long',\n            'summary/short',\n            'summary/tiny'}),\n frozenset({'bbcid', 'is_factual', 'summary', 'system', 'worker_id'}),\n frozenset({'input', 'metadata', 'output'}),\n frozenset({'context',\n            'dialog_acts',\n            'dialog_id',\n            'gem_id',\n            'gem_parent_id',\n            'prompt',\n            'references',\n            'service',\n            'target',\n            'turn_id'}),\n frozenset({'date',\n            'gem_id',\n            'gem_parent_id',\n            'references',\n            'target',\n            'text',\n            'title',\n            'topic',\n            'url'}),\n frozenset({'domain', 'source_id', 'source_text', 'targets'}),\n frozenset({'abstract', 'article'}),\n frozenset({'text'}),\n frozenset({'article', 'highlights', 'id'}),\n frozenset({'alignment_score',\n            'chunk',\n            'chunk_id',\n            'turn_end',\n            'turn_start',\n            'turns'}),\n frozenset({'docs', 'id', 'summary'}),\n frozenset({'abstract',\n            'asjc',\n            'author_highlights',\n            'body_text',\n            'keywords',\n            'subjareas',\n            'title'}),\n frozenset({'email_body', 'subject_line'}),\n frozenset({'concept_set_id',\n            'concepts',\n            'gem_id',\n            'gem_parent_id',\n            'references',\n            'target'}),\n frozenset({'category',\n            'gem_id',\n            'gem_parent_id',\n            'input',\n            'references',\n            'target',\n            'webnlg_id'}),\n frozenset({'article_content', 'date', 'summary', 'title', 'url'}),\n frozenset({'dialogue', 'id', 'summary'}),\n frozenset({'abstract', 'background', 'pmid', 'review_id', 'target', 'title'}),\n frozenset({'paper_id', 'rouge_scores', 'source', 'source_labels', 'target'}),\n frozenset({'dart_id',\n            'gem_id',\n            'gem_parent_id',\n            'references',\n            'subtree_was_extended',\n            'target',\n            'target_sources',\n            'tripleset'}),\n frozenset({'discipline', 'paper_id', 'src', 'tgt', 'title'}),\n frozenset({'example_id',\n            'gem_id',\n            'gem_parent_id',\n            'highlighted_cells',\n            'overlap_subset',\n            'references',\n            'sentence_annotations',\n            'table',\n            'table_page_title',\n            'table_section_text',\n            'table_section_title',\n            'table_webpage_url',\n            'target',\n            'totto_id'}),\n frozenset({'documents',\n            'num_comments',\n            'score',\n            'title',\n            'tldr',\n            'ups',\n            'upvote_ratio'}),\n frozenset({'accession_id',\n            'back',\n            'body',\n            'box',\n            'chemical',\n            'citation',\n            'code',\n            'conclusion',\n            'discussion',\n            'figure',\n            'footnote',\n            'formula',\n            'front',\n            'glossary',\n            'graphic',\n            'introduction',\n            'last_updated',\n            'license',\n            'media',\n            'methods',\n            'n_references',\n            'package_file',\n            'pmid',\n            'quote',\n            'results',\n            'retracted',\n            'supplementary',\n            'table',\n            'unknown_pub'}),\n frozenset({'date',\n            'id',\n            'program',\n            'speaker',\n            'summary',\n            'title',\n            'url',\n            'utt'}),\n frozenset({'summary', 'text', 'title'}),\n frozenset({'abstract', 'article', 'embeddings'}),\n frozenset({'gem_id', 'gem_parent_id', 'references', 'source', 'target'}),\n frozenset({'author',\n            'body',\n            'content',\n            'id',\n            'normalizedBody',\n            'subreddit',\n            'subreddit_id',\n            'summary'}),\n frozenset({'ic',\n            'paper_id',\n            'rouge_scores',\n            'source',\n            'source_labels',\n            'target'}),\n frozenset({'document',\n            'gem_id',\n            'gem_parent_id',\n            'references',\n            'target',\n            'xsum_id'}),\n frozenset({'gem_id',\n            'gem_parent_id',\n            'references',\n            'source',\n            'source_aligned',\n            'target',\n            'target_aligned'}),\n frozenset({'report', 'summary'}),\n frozenset({'document', 'summary'}),\n frozenset({'customer_id',\n            'helpful_votes',\n            'marketplace',\n            'product_category',\n            'product_id',\n            'product_parent',\n            'product_title',\n            'review_body',\n            'review_date',\n            'review_headline',\n            'review_id',\n            'star_rating',\n            'total_votes',\n            'verified_purchase',\n            'vine'}),\n frozenset({'review_sents', 'summaries'}),\n frozenset({'bbcid',\n            'hallucinated_span_end',\n            'hallucinated_span_start',\n            'hallucination_type',\n            'summary',\n            'system',\n            'worker_id'}),\n frozenset({'abstract',\n            'authors',\n            'categories',\n            'comments',\n            'doi',\n            'id',\n            'journal-ref',\n            'report-no',\n            'submitter',\n            'title',\n            'versions'}),\n frozenset({'gt', 'performers', 'task', 'transcriptions'})}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_configurations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T03:31:49.327692Z",
     "start_time": "2023-09-17T03:31:49.323689200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Look out by description for news articles and licenses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b18d4bdeb44d40b3a6ccb23964a76e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce654741c8e24ccfb288ec44372a3e35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_keywords = [\"news\", \"headline\", \"journalism\", \"report\", \"media\", \"press\"]\n",
    "\n",
    "news_related_datasets = []\n",
    "\n",
    "for item in my_list:\n",
    "    try:\n",
    "        config_names = datasets.get_dataset_config_names(item.id, download_mode='reuse_cache_if_exists')\n",
    "        if config_names:\n",
    "            for config_name in config_names:\n",
    "                config = datasets.get_dataset_config_info(item.id, config_name, download_mode='reuse_cache_if_exists')\n",
    "                if config.features is not None:\n",
    "                    if any(keyword.lower() in config.description.lower() for keyword in news_keywords):\n",
    "                        if item.id not in news_related_datasets:\n",
    "                            news_related_datasets[item.id] = []\n",
    "                        news_related_datasets[item.id].append(config)\n",
    "        else:\n",
    "            config = datasets.get_dataset_config_info(item.id, download_mode='reuse_cache_if_exists')\n",
    "            if config.features is not None:\n",
    "                if any(keyword.lower() in config.description.lower() for keyword in news_keywords):\n",
    "                    if item.id not in news_related_datasets:\n",
    "                        news_related_datasets[item.id] = []\n",
    "                    news_related_datasets[item.id].append(config)\n",
    "    except Exception:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T04:07:47.961891700Z",
     "start_time": "2023-09-17T04:03:16.957999400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_keywords = [\"news\", \"headline\", \"journalism\", \"report\", \"media\", \"press\"]\n",
    "filt = DatasetFilter(task_categories=args.task_categories.summarization, language='en', multilinguality=\"monolingual\")\n",
    "my_list = api.list_datasets(filter=filt)\n",
    "\n",
    "# Initialize as a dictionary\n",
    "news_related_datasets = {}\n",
    "\n",
    "for item in my_list:\n",
    "    try:\n",
    "        config_names = datasets.get_dataset_config_names(item.id, download_mode='reuse_cache_if_exists')\n",
    "        config_names = config_names if config_names else [None]\n",
    "\n",
    "        for config_name in config_names:\n",
    "            config = datasets.get_dataset_config_info(item.id, config_name, download_mode='reuse_cache_if_exists')\n",
    "            if config and config.features and config.description:\n",
    "                if any(keyword.lower() in config.description.lower() for keyword in news_keywords):\n",
    "                    if item.id not in news_related_datasets:\n",
    "                        news_related_datasets[item.id] = {'configs': [], 'license': None}\n",
    "                    news_related_datasets[item.id]['configs'].append(config)\n",
    "\n",
    "        if item.id in news_related_datasets and news_related_datasets[item.id]['configs']:\n",
    "            news_related_datasets[item.id]['license'] = get_tag_value('license', item)\n",
    "    except Exception as e:  # Consider catching specific exceptions\n",
    "        print(f\"Exception encountered for item id {item.id if item and item.id else 'Unknown'}: {e}\")\n",
    "\n",
    "# Check if news_related_datasets is empty\n",
    "if not any(val['configs'] for val in news_related_datasets.values()):\n",
    "    print(\"No news-related datasets found.\")\n",
    "else:\n",
    "    print(f\"Found {len(news_related_datasets)} news-related datasets.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception encountered for item id GroNLP/ik-nlp-22_slp: Python int too large to convert to C long\n",
      "Exception encountered for item id ccdv/cnn_dailymail: The split names could not be parsed from the dataset config.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/33.3k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "837abb8aa21e4cbfafe3e955e0e35e6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception encountered for item id shanya/crd3: The split names could not be parsed from the dataset config.\n",
      "Exception encountered for item id knkarthick/AMI: Couldn't find a dataset script at C:\\Users\\PC\\PycharmProjects\\TLDR\\scripts\\knkarthick\\AMI\\AMI.py or any data file in the same directory. Couldn't find 'knkarthick/AMI' on the Hugging Face Hub either: FileNotFoundError: Dataset 'knkarthick/AMI' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.\n",
      "Exception encountered for item id RUCAIBox/Summarization: Couldn't find a dataset script at C:\\Users\\PC\\PycharmProjects\\TLDR\\scripts\\RUCAIBox\\Summarization\\Summarization.py or any data file in the same directory. Couldn't find 'RUCAIBox/Summarization' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in RUCAIBox/Summarization. \n",
      "Exception encountered for item id NavidVafaei/rottentomato01: array() takes at least 1 positional argument (0 given)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42e1830b0a6d493587dd7f4a94d4d045"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8b484e6c9a54a2fbbaebe83254168f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102 news-related datasets.\n"
     ]
    }
   ],
   "source": [
    "news_keywords = [\"news\", \"headline\", \"journalism\", \"report\", \"media\", \"press\"]\n",
    "filt = DatasetFilter(task_categories=args.task_categories.summarization, language='en', multilinguality=\"monolingual\")\n",
    "my_list = api.list_datasets(filter=filt)\n",
    "# Initialize as a dictionary\n",
    "news_related_datasets = {}\n",
    "\n",
    "for item in my_list:\n",
    "    try:\n",
    "        config_names = datasets.get_dataset_config_names(item.id, download_mode='reuse_cache_if_exists')\n",
    "        if item.id not in news_related_datasets:\n",
    "            news_related_datasets[item.id] = {'configs': [], 'license': None}\n",
    "\n",
    "        if config_names:\n",
    "            for config_name in config_names:\n",
    "                config = datasets.get_dataset_config_info(item.id, config_name, download_mode='reuse_cache_if_exists')\n",
    "                if config and config.features and config.description:\n",
    "                    if any(keyword.lower() in config.description.lower() for keyword in news_keywords):\n",
    "                        news_related_datasets[item.id]['configs'].append(config)\n",
    "        else:\n",
    "            config = datasets.get_dataset_config_info(item.id, download_mode='reuse_cache_if_exists')\n",
    "            if config and config.features and config.description:\n",
    "                if any(keyword.lower() in config.description.lower() for keyword in news_keywords):\n",
    "                    news_related_datasets[item.id]['configs'].append(config)\n",
    "\n",
    "        if news_related_datasets[item.id]['configs']:\n",
    "            news_related_datasets[item.id]['license'] = get_tag_value('license', item)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception encountered for item id {item.id if item and item.id else 'Unknown'}: {e}\")\n",
    "\n",
    "# Check if news_related_datasets is empty\n",
    "if not any(val['configs'] for val in news_related_datasets.values()):\n",
    "    print(\"No news-related datasets found.\")\n",
    "else:\n",
    "    print(f\"Found {len(news_related_datasets)} news-related datasets.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T21:15:59.042886900Z",
     "start_time": "2023-09-17T21:10:43.818762200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_related_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "for dataset_id, configs in news_related_datasets.items():\n",
    "    for config in configs:\n",
    "        if hasattr(config, 'features'):\n",
    "            if len(config.features) < 4:\n",
    "                print(f\"{dataset_id}: {config.features.keys()}, size: {config.dataset_size}\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T05:10:47.112033100Z",
     "start_time": "2023-09-17T05:10:47.109770100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Output the IDs of news-related datasets along with their configurations\n",
    "for dataset_id, config_names in news_related_datasets:\n",
    "    if config_names:\n",
    "        print(f\"News-related dataset: {dataset_id}, Configurations: {', '.join(config_names)}\")\n",
    "    else:\n",
    "        print(f\"News-related dataset: {dataset_id}, No specific configuration.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T04:08:13.399556Z",
     "start_time": "2023-09-17T04:08:13.397559400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting tags from datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# List of acceptable licenses\n",
    "acceptable_licenses = ['MIT', 'Apache-2.0', 'CC0', 'CC BY', 'CC BY-SA']\n",
    "\n",
    "# Loop through each dataset to check license and configurations\n",
    "for dataset_id in my_list:\n",
    "    # You can get information about the dataset without downloading it using `load_dataset` with `download_mode='reuse_cache_if_exists'`\n",
    "    dataset = load_dataset(dataset_id.id, download_mode='reuse_cache_if_exists', split='train[:0]')  # Split set to 'train[:0]' to download meta-data only\n",
    "    dataset_info = dataset.info\n",
    "\n",
    "    # Check license\n",
    "    license_type = dataset_info.license  # Update this line based on how license information is stored in `dataset_info`\n",
    "    if license_type not in acceptable_licenses:\n",
    "        print(f\"Skipping {dataset_id.id} due to incompatible license: {license_type}\")\n",
    "        continue\n",
    "\n",
    "    # Check configurations\n",
    "    config_names = dataset_info.config_name  # Replace this with the appropriate way to get configuration names\n",
    "    print(f\"Available configurations for {dataset_id.id}: {config_names}\")\n",
    "\n",
    "    # Decide whether to download the dataset\n",
    "    # user_input = input(f\"Do you want to download {dataset_id.id}? (y/n): \")\n",
    "    # if user_input.lower() == 'y':\n",
    "    #     # Your download code here. For example:\n",
    "    #     dataset = load_dataset(dataset_id, split='train')  # This will download the dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Your existing acceptable_licenses and my_list\n",
    "acceptable_licenses = ['MIT', 'Apache-2.0', 'CC0', 'CC BY', 'CC BY-SA']\n",
    "\n",
    "config_dropdowns = []\n",
    "\n",
    "# Loop through each dataset to check license and configurations\n",
    "for dataset_id in my_list:\n",
    "    dataset = load_dataset(dataset_id.id, download_mode='reuse_cache_if_exists', split='train[:0]')\n",
    "    dataset_info = dataset.info\n",
    "\n",
    "    # Check license\n",
    "    license_type = dataset_info.license\n",
    "    if license_type not in acceptable_licenses:\n",
    "        print(f\"Skipping {dataset_id.id} due to incompatible license: {license_type}\")\n",
    "        continue\n",
    "\n",
    "    # Check configurations\n",
    "    config_names = dataset_info.builder_configs  # The attribute might be different; replace accordingly\n",
    "    config_names = [config.name for config in config_names]\n",
    "\n",
    "    if config_names:\n",
    "        print(f\"Available configurations for {dataset_id.id}: {config_names}\")\n",
    "\n",
    "        dropdown = Dropdown(options=config_names, description=dataset_id.id)\n",
    "        config_dropdowns.append(dropdown)\n",
    "        display(dropdown)\n",
    "\n",
    "# Later, you can collect the user selections:\n",
    "selected_configs = [(dropdown.description, dropdown.value) for dropdown in config_dropdowns]\n",
    "\n",
    "# Now you have a list of tuples: [('dataset_id1', 'selected_config1'), ('dataset_id2', 'selected_config2'), ...]\n",
    "# You can proceed to download these selected configurations.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to handle the \"Download\" button click\n",
    "def on_download_click(dataset_id, config_value, license_type):\n",
    "    print(f\"Downloading {dataset_id} with config {config_value} and license {license_type}\")\n",
    "    # Add your download code here\n",
    "\n",
    "# Function to handle the \"Skip\" button click\n",
    "def on_skip_click(dataset_id):\n",
    "    print(f\"Skipping {dataset_id}\")\n",
    "\n",
    "# Initialize Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title('Dataset Downloader')\n",
    "filt = DatasetFilter(task_categories=args.task_categories.summarization, language='en', multilinguality=\"monolingual\")\n",
    "my_list = api.list_datasets(filter=filt)\n",
    "\n",
    "# Loop through each dataset to create UI elements\n",
    "for dataset in my_list:\n",
    "    dataset_id = dataset.id\n",
    "    config_options = datasets.get_dataset_config_names(dataset_id)\n",
    "    license_type = dataset.tags[7]\n",
    "\n",
    "    frame = ttk.Frame(root, padding=\"10\")\n",
    "    frame.grid(row=0, column=0, sticky=(tk.W, tk.E))\n",
    "\n",
    "    ttk.Label(frame, text=f\"Dataset ID: {dataset_id}\").grid(row=0, column=0, sticky=tk.W)\n",
    "    ttk.Label(frame, text=f\"License: {license_type}\").grid(row=1, column=0, sticky=tk.W)\n",
    "\n",
    "    config_value = tk.StringVar(value=config_options[0])\n",
    "    config_combo = ttk.Combobox(frame, textvariable=config_value)\n",
    "    config_combo['values'] = config_options\n",
    "    config_combo.grid(row=2, column=0, sticky=tk.W)\n",
    "\n",
    "    ttk.Button(frame, text=\"Download\", command=lambda dataset_id=dataset_id, config_value=config_value, license_type=license_type: on_download_click(dataset_id, config_value.get(), license_type)).grid(row=3, column=0)\n",
    "    ttk.Button(frame, text=\"Skip\", command=lambda dataset_id=dataset_id: on_skip_click(dataset_id)).grid(row=3, column=1)\n",
    "\n",
    "root.mainloop()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove from dataset in api"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def remove_dataset_from_cache(dataset_name, cache_dir=None):\n",
    "    if cache_dir is None:\n",
    "        # Replace this with the actual default cache directory path if it's different\n",
    "        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "    dataset_path = os.path.join(cache_dir, dataset_name)\n",
    "\n",
    "    if os.path.exists(dataset_path):\n",
    "        # Remove the dataset directory\n",
    "        shutil.rmtree(dataset_path)\n",
    "        print(f\"Successfully removed {dataset_name} from cache.\")\n",
    "    else:\n",
    "        print(f\"Dataset {dataset_name} not found in cache.\")\n",
    "\n",
    "# Example usage:\n",
    "remove_dataset_from_cache('squad')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize an empty set to store unique keys\n",
    "unique_keys = set()\n",
    "\n",
    "# Loop through each DatasetInfo object to collect keys\n",
    "for dataset_info in my_list:\n",
    "    unique_keys.add(dataset_info.id)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
